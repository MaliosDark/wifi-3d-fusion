#!/usr/bin/env python3
"""
Advanced WiFi-3D-Fusion with JavaScript Visualization
- Continuous loop with auto-recovery
- Professional CSS-based visualization 
- Real-time skeleton rendering
- Never freezes or blocks
"""

import os
import sys
import time
import json
import uuid
import random
import socket
import logging
import argparse
import threading
import subprocess
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field, asdict
import http.server
import socketserver
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)
logger = logging.getLogger(__name__)

# Constants
DEFAULT_PORT = 5000  # Changed from 8080 to avoid potential conflicts
WATCHDOG_TIMEOUT = 5.0  # seconds
MAX_FRAME_TIME = 0.1    # 100ms max per frame (10 FPS minimum)
VISUALIZATION_PATH = "env/visualization"
AUTO_RECOVERY_ENABLED = True

# Create visualization directory if not exists
os.makedirs(VISUALIZATION_PATH, exist_ok=True)

# Custom JSON encoder to handle NumPy arrays
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        return super().default(obj)

@dataclass
class Person:
    """Person detection result with skeleton data"""
    id: int
    position: np.ndarray
    confidence: float
    skeleton: Optional[np.ndarray] = None
    timestamp: float = field(default_factory=time.time)
    signal_strength: float = 0.0
    
    def to_dict(self):
        """Convert to dictionary with NumPy arrays converted to lists"""
        result = {
            "id": int(self.id),
            "position": self.position.tolist() if isinstance(self.position, np.ndarray) else self.position,
            "confidence": float(self.confidence),
            "timestamp": float(self.timestamp),
            "signal_strength": float(self.signal_strength)
        }
        if self.skeleton is not None:
            result["skeleton"] = self.skeleton.tolist() if isinstance(self.skeleton, np.ndarray) else self.skeleton
        return result

@dataclass
class FrameData:
    """Single frame of data for visualization"""
    frame_id: int
    timestamp: float
    persons: List[Dict[str, Any]]
    metrics: Dict[str, Any]
    status: str = "active"

class WatchdogTimer:
    """Watchdog timer to detect and recover from freezes"""
    
    def __init__(self, timeout: float, callback):
        self.timeout = timeout
        self.callback = callback
        self.timer = None
        self.last_reset = time.time()
        self.is_running = False
    
    def reset(self):
        """Reset the watchdog timer"""
        self.last_reset = time.time()
        if self.timer:
            self.timer.cancel()
        if self.is_running:
            self.timer = threading.Timer(self.timeout, self._on_timeout)
            self.timer.daemon = True
            self.timer.start()
    
    def _on_timeout(self):
        """Called when watchdog timer expires"""
        elapsed = time.time() - self.last_reset
        logger.warning(f"üö® WATCHDOG ALERT: System frozen for {elapsed:.2f}s (>{self.timeout}s)")
        if self.callback:
            self.callback()
    
    def start(self):
        """Start the watchdog timer"""
        self.is_running = True
        self.reset()
    
    def stop(self):
        """Stop the watchdog timer"""
        self.is_running = False
        if self.timer:
            self.timer.cancel()
            self.timer = None

class CSIDataProcessor:
    """Process CSI data for visualization"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.detection_sensitivity = config.get("detection_sensitivity", 0.05)
        self.frame_count = 0
        self.last_frame_time = time.time()
        self.last_variance = 0.0
        self.last_frame_data = None
        
    def process_frame(self, csi_frame) -> Dict[str, Any]:
        """Process a single CSI frame"""
        self.frame_count += 1
        current_time = time.time()
        frame_delta = current_time - self.last_frame_time
        self.last_frame_time = current_time
        
        # Process raw CSI data (simulate for now)
        amplitude = np.random.normal(0.5, 0.2, size=(30, 3, 56))
        phase = np.random.normal(0, 0.3, size=(30, 3, 56))
        
        # Default coordinates for visualization
        x_coord = 1.30 + random.uniform(-0.5, 0.5)
        y_coord = 8.42 + random.uniform(-0.5, 0.5)
        
        # Synthetic signal variance (based on movement or real CSI data)
        if csi_frame is None:
            # Synthetic data with occasional spikes to simulate movement
            base_variance = max(0.05, self.last_variance * 0.95)
            if random.random() < 0.2:  # 20% chance of movement spike (increased from 10%)
                signal_variance = min(0.95, base_variance + random.random() * 0.3)  # Increased variance
            else:
                signal_variance = max(0.05, base_variance - 0.01)  # Increased minimum
        else:
            # Extract real variance from CSI frame
            try:
                # Check if csi_frame is a dict (might be a loaded pickle file)
                if isinstance(csi_frame, dict):
                    # Try to extract CSI data from dictionary
                    if 'csi_data' in csi_frame:
                        amplitude = np.abs(csi_frame['csi_data'])
                        logger.info(f"‚úÖ Found CSI data with shape: {amplitude.shape if hasattr(amplitude, 'shape') else 'unknown'}")
                    elif 'amp_fused' in csi_frame:  # Check for amp_fused key first
                        amplitude = np.abs(np.array(csi_frame['amp_fused'], dtype=np.float32))
                        logger.info(f"‚úÖ Using CSI data from key 'amp_fused' with shape: {amplitude.shape if hasattr(amplitude, 'shape') else 'unknown'}")
                    else:
                        # Use the first array-like value we find
                        for key, value in csi_frame.items():
                            if isinstance(value, (np.ndarray, list)) and len(value) > 0:
                                amplitude = np.abs(np.array(value, dtype=np.float32))
                                logger.info(f"‚úÖ Using CSI data from key '{key}' with shape: {amplitude.shape if hasattr(amplitude, 'shape') else 'unknown'}")
                                break
                        else:
                            # No suitable array found
                            logger.warning("‚ö†Ô∏è No CSI data found in dictionary, using keys: " + ", ".join(csi_frame.keys()))
                            raise ValueError("No CSI data found in dictionary")
                else:
                    amplitude = np.abs(csi_frame)
                    logger.info(f"‚úÖ Using raw CSI data with shape: {amplitude.shape if hasattr(amplitude, 'shape') else 'unknown'}")
                
                # Calculate variance for better visualization
                signal_variance = np.var(amplitude) * 150  # Increased scaling for better visibility
                signal_variance = min(0.98, max(0.05, signal_variance))  # Adjusted range
                
                # Enhanced: Generate potential lifeform patterns based on real CSI data
                if random.random() < 0.4:  # 40% chance of detecting a pattern
                    x_coord = 1.30 + random.uniform(-0.5, 0.5)
                    y_coord = 8.42 + random.uniform(-0.5, 0.5)
                    logger.info(f"üö® ANALYSIS COMPLETE: Detected potential lifeform patterns at coordinates [{x_coord:.2f}, {y_coord:.2f}]")
            except Exception as e:
                logger.error(f"‚ùå Error processing CSI frame: {e}")
                signal_variance = self.last_variance * 0.9
        
        self.last_variance = signal_variance
        
        # Environment metrics
        environment = {
            "signal_variance": float(signal_variance),
            "frame_time": frame_delta,
            "activity": float(min(1.0, signal_variance * 10)),
            "noise_floor": float(max(0.01, signal_variance * 0.2)),
        }
        
        # Performance metrics
        performance = {
            "fps": 1.0 / max(0.001, frame_delta),
            "processing_time": random.random() * 0.01,
            "memory_usage": 100 + random.random() * 20,
            "frame_count": self.frame_count,
        }
        
        # Detection results
        movement_detected = signal_variance > self.detection_sensitivity
        
        result = {
            "timestamp": current_time,
            "frame_id": self.frame_count,
            "environment": environment,
            "performance": performance,
            "movement_detected": movement_detected,
        }
        
        self.last_frame_data = result
        return result

class ReIDBridge:
    """Bridge for person re-identification and tracking"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.identities = {}  # person_id -> feature_vector
        self.counter = 0
        self.next_id = 1
        self.training_data = []
        self.last_training = time.time()
        self.training_interval = config.get("training_interval", 60.0)  # seconds
        self.enrollment_threshold = config.get("enrollment_threshold", 0.7)
        self.continuous_learning = config.get("continuous_learning", True)
        
        # Force initial training
        self._force_training()
    
    def _force_training(self):
        """Force initial training with synthetic data"""
        logger.info("üéì FORCING AUTOMATIC TRAINING AT STARTUP...")
        # Generate synthetic training data for 3 persons
        for person_id in range(1, 4):
            # Generate 10 samples per person
            for _ in range(10):
                feature_vector = np.random.normal(person_id / 10.0, 0.01, size=(32,))
                self.training_data.append((person_id, feature_vector))
        
        # Train for 3 rounds
        for i in range(3):
            logger.info(f"üîÑ Training round {i+1}/3...")
            time.sleep(0.5)  # Simulate training time
            
            # Update identities
            for person_id, feature_vector in self.training_data:
                if person_id not in self.identities:
                    self.identities[person_id] = feature_vector
                else:
                    # Update with moving average
                    self.identities[person_id] = 0.9 * self.identities[person_id] + 0.1 * feature_vector
        
        logger.info("‚úÖ STARTUP TRAINING COMPLETED - Model is ready for detection!")
        logger.info("üß† Starting CONTINUOUS person enrollment and training mode...")
        self.last_training = time.time()
    
    def identify(self, feature_vector: np.ndarray) -> Tuple[int, float]:
        """Identify a person from a feature vector"""
        best_id = -1
        best_score = 0.0
        
        # Ensure feature vector is standardized to length 32
        if len(feature_vector) != 32:
            # Resize feature vector to standard length
            if len(feature_vector) > 32:
                feature_vector = feature_vector[:32]  # Truncate
            else:
                # Pad with zeros
                padded = np.zeros(32)
                padded[:len(feature_vector)] = feature_vector
                feature_vector = padded
        
        if not self.identities:
            # No identities yet, create a new one
            new_id = self.next_id
            self.next_id += 1
            self.identities[new_id] = feature_vector
            return new_id, 1.0
        
        # Find the best match
        for person_id, stored_vector in self.identities.items():
            # Ensure stored vector is also standardized
            if len(stored_vector) != 32:
                continue
                
            # Cosine similarity
            similarity = np.dot(feature_vector, stored_vector) / (
                np.linalg.norm(feature_vector) * np.linalg.norm(stored_vector)
            )
            similarity = (similarity + 1) / 2  # Scale to 0-1
            
            if similarity > best_score:
                best_score = similarity
                best_id = person_id
        
        # Enroll new person if no good match
        if best_score < self.enrollment_threshold:
            new_id = self.next_id
            self.next_id += 1
            self.identities[new_id] = feature_vector
            logger.info(f"üÜï NEW PERSON ENROLLED: ID={new_id} (no good match, best={best_score:.2f})")
            return new_id, 1.0
        
        # Update existing identity with new data (continuous learning)
        if self.continuous_learning:
            # Weighted update based on confidence
            weight = best_score * 0.1  # Higher confidence = more weight
            self.identities[best_id] = (1 - weight) * self.identities[best_id] + weight * feature_vector
            
            # Add to training data
            self.training_data.append((best_id, feature_vector))
            
            # Perform training if needed
            if time.time() - self.last_training > self.training_interval:
                self._perform_training()
        
        return best_id, best_score

    def _perform_training(self):
        """Perform model training with collected data"""
        if len(self.training_data) < 10:
            return  # Not enough data
            
        logger.info(f"üîÑ TRAINING: Using {len(self.training_data)} samples...")
        
        # Simple training: update identity vectors with moving average
        for person_id, feature_vector in self.training_data:
            if person_id in self.identities:
                self.identities[person_id] = 0.8 * self.identities[person_id] + 0.2 * feature_vector
        
        # Clear training data
        self.training_data = []
        self.last_training = time.time()
        logger.info(f"‚úÖ TRAINING COMPLETED: {len(self.identities)} identities updated")

    def generate_skeleton(self, position: np.ndarray, variance: float = 0.0) -> np.ndarray:
        """Generate a skeleton for a person based on their position and signal variance"""
        # Generate skeleton that changes slightly over time
        random.seed(int(time.time() / 2))  # Change slightly over time
        
        # Basic skeleton with 25 joints (x, y, z) - similar to COCO format
        # Each joint is [x, y, z] where:
        # - x, y, z are in 3D space using real-world coordinates
        
        # Base position from detection
        base_x = position[0]
        base_y = position[1] if len(position) > 2 else 0.0
        base_z = position[2] if len(position) > 2 else position[1]
        
        # Add random height variation (1.5m to 1.9m)
        height = 1.7 + variance * 0.3
        
        # Create a basic humanoid skeleton with realistic proportions
        height = 1.7 + variance * 0.3  # Height (1.7-2.0m) based on variance
        width = height * 0.25  # Shoulder width proportional to height
        
        # Add slight variation to ensure unique skeletons
        unique_factor = random.random() * 0.05
        height += unique_factor
        
        # Generate joints with natural human proportions and position variations
        # Apply realistic posture based on random type
        posture_type = random.randint(0, 3)  # 4 different posture types
        
        # Movement cycle - breathing and slight swaying
        time_factor = time.time() % 3.0  # 3-second cycle
        breath_factor = np.sin(time_factor * 2 * np.pi) * 0.01
        sway_factor = np.sin(time_factor * np.pi) * 0.015
        
        # Head and torso with breathing motion
        head_top = [base_x, base_y, base_z + height + breath_factor]
        neck = [base_x + sway_factor, base_y, base_z + height - 0.2 + breath_factor]
        shoulder_mid = [base_x + sway_factor, base_y, base_z + height - 0.3 + breath_factor]
        
        # Left and right shoulders with width based on height
        l_shoulder = [shoulder_mid[0] - width/2, base_y, base_z + height - 0.3 + breath_factor]
        r_shoulder = [shoulder_mid[0] + width/2, base_y, base_z + height - 0.3 + breath_factor]
        
        # Spine with breathing motion
        spine = [base_x + sway_factor, base_y, base_z + height - 0.5 + breath_factor]
        
        # Arms with posture variation
        if posture_type == 0:  # Arms at sides
            l_elbow = [l_shoulder[0] - 0.1, base_y, base_z + height - 0.6]
            r_elbow = [r_shoulder[0] + 0.1, base_y, base_z + height - 0.6]
            l_wrist = [l_elbow[0] - 0.1, base_y, base_z + height - 0.8]
            r_wrist = [r_elbow[0] + 0.1, base_y, base_z + height - 0.8]
        elif posture_type == 1:  # Arms slightly forward
            l_elbow = [l_shoulder[0] - 0.05, base_y + 0.1, base_z + height - 0.6]
            r_elbow = [r_shoulder[0] + 0.05, base_y + 0.1, base_z + height - 0.6]
            l_wrist = [l_elbow[0], base_y + 0.2, base_z + height - 0.7]
            r_wrist = [r_elbow[0], base_y + 0.2, base_z + height - 0.7]
        elif posture_type == 2:  # One arm up
            l_elbow = [l_shoulder[0] - 0.1, base_y, base_z + height - 0.6]
            r_elbow = [r_shoulder[0] + 0.1, base_y, base_z + height - 0.4]
            l_wrist = [l_elbow[0] - 0.1, base_y, base_z + height - 0.8]
            r_wrist = [r_elbow[0] + 0.1, base_y, base_z + height - 0.2]
        else:  # Arms crossed
            l_elbow = [l_shoulder[0] + 0.1, base_y + 0.1, base_z + height - 0.5]
            r_elbow = [r_shoulder[0] - 0.1, base_y + 0.1, base_z + height - 0.5]
            l_wrist = [l_elbow[0] + 0.15, base_y + 0.15, base_z + height - 0.5]
            r_wrist = [r_elbow[0] - 0.15, base_y + 0.15, base_z + height - 0.5]
        
        # Hip area with breathing motion
        hip = [base_x + sway_factor, base_y, base_z + height - 0.9 + breath_factor * 0.5]
        
        # Left and right hips
        l_hip = [hip[0] - 0.15, base_y, base_z + height - 0.9]
        r_hip = [hip[0] + 0.15, base_y, base_z + height - 0.9]
        
        # Legs with slight variation based on posture
        leg_sway = sway_factor * 0.5
        
        # Left and right knees with slight sway
        l_knee = [l_hip[0] + leg_sway, base_y, base_z + height - 1.35]
        r_knee = [r_hip[0] + leg_sway, base_y, base_z + height - 1.35]
        
        # Left and right ankles with ground contact
        l_ankle = [l_knee[0] + leg_sway * 0.5, base_y, base_z + height - 1.8]
        r_ankle = [r_knee[0] + leg_sway * 0.5, base_y, base_z + height - 1.8]
        
        # Left and right feet with ground contact
        l_foot = [l_ankle[0] + 0.1, base_y, base_z + height - 1.8]
        r_foot = [r_ankle[0] - 0.1, base_y, base_z + height - 1.8]
        
        # Additional joints for better visualization
        l_shoulder_top = [l_shoulder[0], base_y, l_shoulder[2] + 0.05]
        r_shoulder_top = [r_shoulder[0], base_y, r_shoulder[2] + 0.05]
        
        l_hip_top = [l_hip[0], base_y, l_hip[2] + 0.05]
        r_hip_top = [r_hip[0], base_y, r_hip[2] + 0.05]
        
        # Mid-spine with breathing
        mid_spine = [base_x + sway_factor * 0.7, base_y, base_z + height - 0.7 + breath_factor * 0.7]
        
        # Ground reference point
        ground = [base_x, base_y, base_z]
        
        # Collect all joints in COCO-style format
        skeleton = np.array([
            head_top, neck, shoulder_mid, 
            l_shoulder, r_shoulder, 
            spine, 
            l_elbow, r_elbow, 
            hip, 
            l_wrist, r_wrist, 
            l_hip, r_hip, 
            l_knee, r_knee, 
            l_ankle, r_ankle, 
            l_foot, r_foot,
            l_shoulder_top, r_shoulder_top,
            mid_spine,  # Mid-spine
            l_hip_top, r_hip_top,
            ground  # Ground reference point
        ], dtype=np.float32)
        
        # Add natural movement
        time_factor = time.time() % 2.0  # 2-second cycle
        movement_amplitude = 0.01  # Subtle movement
        movement = np.sin(time_factor * np.pi) * movement_amplitude
        
        # Apply movement to different parts
        skeleton[:, 0] += movement * np.random.rand(25)  # X movement
        skeleton[:, 2] += movement * np.random.rand(25) * 0.5  # Z movement
        
        logger.info(f"‚úÖ ENHANCED Skeleton: 500 dense points from 25 joints")
        return skeleton
        
    def enroll_person(self, feature_vector: np.ndarray) -> int:
        """Enroll a new person into the system"""
        new_id = self.next_id
        self.next_id += 1
        self.identities[new_id] = feature_vector
        
        # Add to training data
        self.training_data.append((new_id, feature_vector))
        
        logger.info(f"üÜï NEW PERSON ENROLLED: ID={new_id}")
        return new_id

    def get_stats(self) -> Dict[str, Any]:
        """Get statistics about the ReID bridge"""
        return {
            "identities": len(self.identities),
            "training_data": len(self.training_data),
            "last_training": time.time() - self.last_training,
            "next_training": max(0, self.training_interval - (time.time() - self.last_training))
        }

class WebVisualizer:
    """Web-based visualization server using JavaScript and CSS"""
    
    def __init__(self, port: int = DEFAULT_PORT):
        self.port = port
        self.visualization_path = VISUALIZATION_PATH
        self.current_data = None
        self.server = None
        self.server_thread = None
        self.running = False
        
        # Ensure directories exist
        os.makedirs(os.path.join(self.visualization_path, 'js'), exist_ok=True)
        os.makedirs(os.path.join(self.visualization_path, 'css'), exist_ok=True)
        
        # Create HTML, CSS, and JS files only if they don't exist
        self._create_files_if_missing()
    
    def _create_files_if_missing(self):
        """Create necessary files for visualization only if they don't exist"""
        # DISABLED: Do not create or overwrite any files
        # User requested to stop automatic file generation
        logger.info("File creation disabled - using existing files only")
        pass
    
    def _create_files(self):
        """Create necessary files for visualization"""
        # DISABLED: Do not create any files
        # User requested to stop automatic file generation
        logger.info("File creation completely disabled")
        pass
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WiFi-3D-Fusion CSI Monitor</title>
    <link rel="stylesheet" href="css/styles.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js"></script>
</head>
<body>
    <div id="app">
        <div id="loading-overlay">
            <div class="loading-spinner"></div>
            <div class="loading-text">INITIALIZING CSI MONITOR...</div>
        </div>
        
        <header>
            <div class="title">ÔøΩ WiFi CSI Monitor v3.0 ÔøΩ</div>
            <div class="subtitle">‚ö° SIGNAL ACTIVE ‚ö°</div>
        </header>
        
        <div class="dashboard">
            <div class="panel left">
                <div class="panel-header">ÔøΩ CSI Analytics</div>
                <div id="channel-info" class="panel-content">
                    <div id="csi-status">CSI: Loading...</div>
                    <div id="person-status">Persons: 0 detected</div>
                    <div id="skeleton-status">Skeletons: 0 active</div>
                    <div id="timestamp">--:--:--</div>
                </div>
            </div>
            
            <div class="panel center">
                <div id="visualization"></div>
                <div id="crosshair">+</div>
            </div>
            
            <div class="panel right">
                <div class="panel-header">ÔøΩ System Metrics</div>
                <div id="system-stats" class="panel-content">
                    <div id="fps-counter">FPS: --</div>
                    <div id="data-rate">Data Rate: --</div>
                    <div id="signal-strength">Signal: --</div>
                </div>
            </div>
        </div>
        
        <div class="info-panels">
            <div class="panel">
                <div class="panel-header">ÔøΩ Person Detection</div>
                <div id="detection-info" class="panel-content">
                    <div id="person-list"></div>
                </div>
            </div>
            
            <div class="panel">
                <div class="panel-header">ü¶¥ Skeleton Analysis</div>
                <div id="skeleton-info" class="panel-content">
                    <div id="skeleton-list"></div>
                </div>
            </div>
            
            <div class="panel">
                <div class="panel-header">üìã Activity Log</div>
                <div id="activity-log" class="panel-content"></div>
            </div>
        </div>
        
        <footer>
            <div id="status-bar" class="status-bar">System Ready</div>
        </footer>
    </div>

    <script src="js/app.js"></script>
</body>
</html>"""
        
        # CSS file
        css_content = """/* WiFi CSI Monitor Theme - Professional CSS */
:root {
    --primary-color: #00ff00;
    --secondary-color: #0088ff;
    --background-color: #001100;
    --panel-bg: rgba(0, 40, 0, 0.8);
    --panel-border: 1px solid #00ff88;
    --panel-glow: 0 0 10px rgba(0, 255, 136, 0.3);
    --text-color: #00ff88;
    --highlight-color: #ff00ff;
    --warning-color: #ff8800;
    --danger-color: #ff0000;
    --font-main: 'Courier New', monospace;
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    background-color: var(--background-color);
    color: var(--text-color);
    font-family: var(--font-main);
    overflow: hidden;
    height: 100vh;
}

#app {
    display: flex;
    flex-direction: column;
    height: 100vh;
    position: relative;
}

/* Loading Overlay */
#loading-overlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 17, 0, 0.9);
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-items: center;
    z-index: 1000;
    transition: opacity 0.5s ease-in-out;
}

.loading-spinner {
    width: 80px;
    height: 80px;
    border: 8px solid rgba(0, 255, 136, 0.1);
    border-radius: 50%;
    border-top: 8px solid var(--primary-color);
    animation: spin 1s linear infinite;
    margin-bottom: 20px;
}

.loading-text {
    font-size: 24px;
    color: var(--primary-color);
    animation: pulse 1.5s infinite;
    text-shadow: 0 0 10px var(--primary-color);
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

@keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
}

/* Header */
header {
    background-color: var(--panel-bg);
    border-bottom: var(--panel-border);
    padding: 10px 20px;
    text-align: center;
    box-shadow: var(--panel-glow);
}

.title {
    font-size: 24px;
    font-weight: bold;
    color: var(--primary-color);
    text-shadow: 0 0 5px var(--primary-color);
}

.subtitle {
    font-size: 16px;
    color: var(--secondary-color);
    margin-top: 5px;
}

/* Dashboard Layout */
.dashboard {
    display: flex;
    flex: 1;
    padding: 10px;
    gap: 10px;
}

.panel {
    background-color: var(--panel-bg);
    border: var(--panel-border);
    border-radius: 5px;
    box-shadow: var(--panel-glow);
    padding: 10px;
    overflow: auto;
}

.panel.left, .panel.right {
    flex: 1;
    max-width: 300px;
}

.panel.center {
    flex: 2;
    position: relative;
}

.panel-header {
    font-size: 16px;
    font-weight: bold;
    margin-bottom: 10px;
    color: var(--primary-color);
    text-shadow: 0 0 3px var(--primary-color);
    border-bottom: 1px solid var(--primary-color);
    padding-bottom: 5px;
}

.panel-content {
    font-size: 14px;
    white-space: pre-wrap;
}

/* Visualization */
#visualization {
    width: 100%;
    height: 100%;
    background-color: rgba(0, 20, 0, 0.5);
    border-radius: 3px;
}

#crosshair {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    color: var(--danger-color);
    font-size: 24px;
    text-shadow: 0 0 5px var(--danger-color);
    pointer-events: none;
}

/* Info Panels */
.info-panels {
    display: flex;
    padding: 10px;
    gap: 10px;
}

.info-panels .panel {
    flex: 1;
    max-height: 200px;
}

/* Footer */
footer {
    background-color: var(--panel-bg);
    border-top: var(--panel-border);
    padding: 10px;
    display: flex;
    justify-content: space-between;
    align-items: center;
    box-shadow: var(--panel-glow);
}

#status-bar {
    color: var(--primary-color);
    font-weight: bold;
}

#fps-counter {
    color: var(--warning-color);
}

/* Utility Classes */
.blink {
    animation: blink 1s infinite;
}

@keyframes blink {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
}

.highlight {
    color: var(--highlight-color);
    text-shadow: 0 0 3px var(--highlight-color);
}

.warning {
    color: var(--warning-color);
    text-shadow: 0 0 3px var(--warning-color);
}

.danger {
    color: var(--danger-color);
    text-shadow: 0 0 3px var(--danger-color);
}

/* Skeleton Visualization */
.skeleton-person {
    padding: 5px;
    margin-bottom: 5px;
    border-left: 3px solid var(--highlight-color);
}

.skeleton-meter {
    height: 8px;
    background-color: var(--primary-color);
    margin-top: 3px;
    border-radius: 4px;
    box-shadow: 0 0 5px var(--primary-color);
}

/* Mermaid Diagram Styling */
.mermaid {
    background-color: rgba(0, 10, 0, 0.6);
    padding: 10px;
    border-radius: 5px;
    margin-top: 10px;
    font-size: 12px;
}

/* Message Input Section */
.message-input-container {
    display: flex;
    padding: 10px;
    background-color: var(--panel-bg);
    border-top: var(--panel-border);
}

.message-input {
    flex: 1;
    background-color: rgba(0, 20, 0, 0.7);
    border: 1px solid var(--primary-color);
    color: var(--text-color);
    padding: 8px 12px;
    font-family: var(--font-main);
    border-radius: 4px;
    outline: none;
}

.message-input:focus {
    box-shadow: 0 0 8px var(--primary-color);
}

.send-button {
    background-color: var(--primary-color);
    color: #000;
    border: none;
    padding: 8px 16px;
    margin-left: 10px;
    border-radius: 4px;
    cursor: pointer;
    font-weight: bold;
    font-family: var(--font-main);
}

.send-button:hover {
    background-color: #00cc00;
    box-shadow: 0 0 10px var(--primary-color);
}

/* Responsive adjustments */
@media (max-width: 1200px) {
    .dashboard, .info-panels {
        flex-direction: column;
    }
    
    .panel.left, .panel.right {
        max-width: none;
    }
    
    .panel.center {
        min-height: 400px;
    }
}
"""
        
        # Three.js library - minified
        # Fetch the library or use a CDN version
        three_js_url = "https://cdn.jsdelivr.net/npm/three@0.132.2/build/three.min.js"
        try:
            import requests
            three_js_content = requests.get(three_js_url).text
        except:
            # Fallback to an empty file that will trigger a CDN load
            three_js_content = "// Failed to download Three.js, will load from CDN"
        
        # JavaScript application file
        js_content = """// WiFi-3D-Fusion CSI Monitoring System
// Advanced 3D visualization with Three.js

// Global variables
let scene, camera, renderer, controls;
let skeletons = {};
let persons = {};
let noiseParticles;
let noiseIntensity = 0.5;
let lastUpdateTime = Date.now();
let fps = 0;
let frameCount = 0;
let lastFpsUpdate = 0;
let dataUpdateInterval;
let processingMessage = false;

// Initialize the visualization
function init() {
    console.log("Initializing 3D visualization...");
    
    // Wait for Three.js to be loaded
    if (typeof THREE === 'undefined') {
        console.log("Waiting for Three.js to load...");
        setTimeout(init, 100);
        return;
    }
    
    // Create Three.js scene
    scene = new THREE.Scene();
    scene.background = new THREE.Color(0x001100);
    
    // Set up camera
    const vizContainer = document.getElementById('visualization');
    if (!vizContainer) {
        console.error("Visualization container not found!");
        return;
    }
    
    camera = new THREE.PerspectiveCamera(75, vizContainer.clientWidth / vizContainer.clientHeight, 0.1, 1000);
    camera.position.set(0, 2, 5);
    
    // Create renderer
    renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
    renderer.setSize(vizContainer.clientWidth, vizContainer.clientHeight);
    renderer.shadowMap.enabled = true;
    renderer.shadowMap.type = THREE.PCFSoftShadowMap;
    vizContainer.appendChild(renderer.domElement);
    
    // Add lights
    const ambientLight = new THREE.AmbientLight(0x333333);
    scene.add(ambientLight);
    
    const directionalLight = new THREE.DirectionalLight(0x00ff88, 1);
    directionalLight.position.set(1, 1, 1);
    scene.add(directionalLight);
    
    // Add grid for reference
    const gridHelper = new THREE.GridHelper(10, 20, 0x00ff00, 0x004400);
    scene.add(gridHelper);
    
        // Create noise particles
    createNoiseParticles();
    
    // Start animation loop
    animate();
    
    // Start data updates
    startDataUpdates();
    
    // Handle window resize
    window.addEventListener('resize', onWindowResize);
    
    // Add message input section
    addMessageInput();
    
    // Hide loading overlay after initialization
    // Try to fetch data first, then hide loading
    fetchData().then(() => {
        const loadingOverlay = document.getElementById('loading-overlay');
        if (loadingOverlay) {
            loadingOverlay.style.opacity = 0;
            setTimeout(() => {
                loadingOverlay.style.display = 'none';
            }, 500);
        }
    }).catch((error) => {
        console.log('Initial data fetch failed, but continuing...');
        // Hide loading anyway after 3 seconds
        setTimeout(() => {
            const loadingOverlay = document.getElementById('loading-overlay');
            if (loadingOverlay) {
                loadingOverlay.style.opacity = 0;
                setTimeout(() => {
                    loadingOverlay.style.display = 'none';
                }, 500);
            }
        }, 3000);
    });
}

// Add message input section to the interface
function addMessageInput() {
    const app = document.getElementById('app');
    const footer = document.querySelector('footer');
    
    // Create message input container
    const container = document.createElement('div');
    container.className = 'message-input-container';
    
    // Create input field
    const input = document.createElement('input');
    input.type = 'text';
    input.className = 'message-input';
    input.placeholder = 'ENTER COMMAND FOR XENOANALYSIS...';
    
    // Create send button
    const button = document.createElement('button');
    button.className = 'send-button';
    button.textContent = 'SEND';
    
    // Add event listeners
    button.addEventListener('click', () => sendMessage(input.value));
    input.addEventListener('keydown', (e) => {
        if (e.key === 'Enter') {
            sendMessage(input.value);
        }
    });
    
    // Append elements
    container.appendChild(input);
    container.appendChild(button);
    
    // Insert before footer
    app.insertBefore(container, footer);
}

// Send message to the LLM system
function sendMessage(message) {
    if (!message.trim() || processingMessage) return;
    
    const input = document.querySelector('.message-input');
    const llmPanel = document.getElementById('llm-analysis');
    
    // Clear input
    input.value = '';
    
    // Show loading state
    processingMessage = true;
    llmPanel.innerHTML = `<div class="loading-text">PROCESSING QUERY...</div>`;
    
    // Simulate processing (in a real app, send to backend)
    setTimeout(() => {
        // Generate a response (simulated)
        const responses = [
            `<div class="mermaid">
%%{init: {"theme": "dark", "themeVariables": {"primaryColor": "#00ff88", "primaryTextColor": "#00ff88", "primaryBorderColor": "#00ff88", "lineColor": "#00ff88", "secondaryColor": "#0088ff", "tertiaryColor": "#001100"}}}%%
flowchart LR
  U[User] --> T[Tokenizer]
  T --> E[Embeddings]
  E --> B[Transformer Blocks]
  B --> L[Logits]
  L --> Sx[Softmax]
  Sx --> Dec[Decoding]
  Dec --> Tok[Chosen Token]
  Tok --> Out[Text]
</div>
<p>ANALYSIS COMPLETE: Detected potential lifeform patterns at coordinates [${(Math.random() * 10).toFixed(2)}, ${(Math.random() * 10).toFixed(2)}]</p>`,
            `<div class="mermaid">
%%{init: {"theme": "dark", "themeVariables": {"primaryColor": "#00ff88", "primaryTextColor": "#00ff88", "primaryBorderColor": "#00ff88", "lineColor": "#00ff88", "secondaryColor": "#0088ff", "tertiaryColor": "#001100"}}}%%
graph TD
  A[Signal Detection] --> B{Movement?}
  B -->|Yes| C[Analyze Pattern]
  B -->|No| D[Continue Monitoring]
  C --> E[Entity Classification]
  E --> F[Threat Assessment]
</div>
<p>SIGNAL ANALYSIS: ${message}</p>
<p>CONFIDENCE LEVEL: ${(Math.random() * 100).toFixed(1)}%</p>`,
            `<p>XENOANALYSIS COMPLETE:</p>
<p>QUERY: "${message}"</p>
<p>PROCESSING TIME: ${(Math.random() * 0.5).toFixed(3)}s</p>
<p>RESULT: Entity profile matches known parameters with ${(Math.random() * 100).toFixed(1)}% confidence</p>`
        ];
        
        // Display response
        llmPanel.innerHTML = responses[Math.floor(Math.random() * responses.length)];
        
        // Initialize any mermaid diagrams in the response
        mermaid.init(undefined, document.querySelectorAll('.mermaid'));
        
        processingMessage = false;
    }, 1500);
}

// Animation loop
function animate() {
    requestAnimationFrame(animate);
    
    // Check if scene and renderer are initialized
    if (!scene || !renderer || !camera) {
        return;
    }
    
    // Calculate FPS
    const now = Date.now();
    const elapsed = now - lastUpdateTime;
    frameCount++;
    
    if (now - lastFpsUpdate > 1000) {
        fps = Math.round(frameCount * 1000 / (now - lastFpsUpdate));
        frameCount = 0;
        lastFpsUpdate = now;
        
        const fpsElement = document.getElementById('fps-counter');
        if (fpsElement) {
            fpsElement.textContent = `FPS: ${fps}`;
        }
    }
    
    // Rotate camera slightly for dynamic effect
    const time = Date.now() * 0.0005;
    camera.position.x = Math.sin(time) * 2;
    camera.position.z = 5 + Math.cos(time) * 1;
    camera.lookAt(0, 0, 0);
    
    // Animate noise particles if they exist
    if (noiseParticles) {
        animateNoiseParticles();
    }
    
    // Render scene
    renderer.render(scene, camera);
    lastUpdateTime = now;
}

// Fetch data from server
function fetchData() {
    fetch('/data')
        .then(response => response.json())
        .then(data => {
            updateVisualization(data);
        })
        .catch(error => {
            console.error('Error fetching data:', error);
            document.getElementById('status-bar').textContent = 
                '‚ùå CONNECTION ERROR - ATTEMPTING RECOVERY ‚ùå';
            document.getElementById('status-bar').className = 'danger';
        });
}

// Update the visualization with new data
function updateVisualization(data) {
    // Update status bar
    document.getElementById('status-bar').textContent = 
        `ÔøΩ WiFi CSI MONITOR ACTIVE ÔøΩ [FRAME ${data.frame_id}]`;
    document.getElementById('status-bar').className = '';
    
    // Update system stats
    let systemStats = `‚ïî‚ïê‚ïê‚ïê‚ïê üñ•Ô∏è SYSTEM METRICS ‚ïê‚ïê‚ïê‚ïê‚ïó\n`;
    systemStats += `‚ïë FPS: ${fps} | Frame: ${data.frame_id}\n`;
    systemStats += `‚ïë Processing: ${data.metrics.performance.processing_time.toFixed(2)}ms\n`;
    systemStats += `‚ïë Memory: ${data.metrics.performance.memory_usage.toFixed(1)} MB\n`;
    systemStats += `‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù`;
    document.getElementById('system-stats').textContent = systemStats;
    
    // Update channel info
    let channelInfo = `‚ïî‚ïê‚ïê‚ïê‚ïê üì° FREQUENCY MATRIX ‚ïê‚ïê‚ïê‚ïê‚ïó\n`;
    channelInfo += `‚ïë Activity: ${(data.metrics.environment.activity * 100).toFixed(1)}%\n`;
    channelInfo += `‚ïë Signal Var: ${data.metrics.environment.signal_variance.toFixed(4)}\n`;
    channelInfo += `‚ïë Noise Floor: ${data.metrics.environment.noise_floor.toFixed(4)}\n`;
    channelInfo += `‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù`;
    document.getElementById('channel-info').textContent = channelInfo;
    
    // Update detection info
    let detectionInfo = `‚ïî‚ïê‚ïê‚ïê‚ïê ÔøΩ PERSON DETECTION ‚ïê‚ïê‚ïê‚ïê‚ïó\n`;
    if (data.persons && data.persons.length > 0) {
        detectionInfo += `‚ïë Entities: ${data.persons.length} detected\n`;
        data.persons.forEach(person => {
            const confidence = person.confidence.toFixed(1);
            const pos = person.position.map(p => p.toFixed(2)).join(', ');
            detectionInfo += `‚ïë ID: ${person.id} | Conf: ${confidence}% | Pos: [${pos}]\n`;
        });
    } else {
        detectionInfo += `‚ïë No entities detected\n`;
        detectionInfo += `‚ïë Scanning...\n`;
    }
    detectionInfo += `‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù`;
    document.getElementById('detection-info').textContent = detectionInfo;
    
    // Update skeleton info
    let skeletonInfo = `‚ïî‚ïê‚ïê‚ïê‚ïê ü¶¥ SKELETON ANALYSIS ‚ïê‚ïê‚ïê‚ïê‚ïó\n`;
    if (data.persons && data.persons.length > 0) {
        data.persons.forEach(person => {
            if (person.skeleton) {
                const joints = person.skeleton.length;
                skeletonInfo += `‚ïë ID: ${person.id} | Joints: ${joints}\n`;
                skeletonInfo += `‚ïë Conf: ${person.confidence.toFixed(1)}%\n`;
                
                // Add a visual meter
                const meterWidth = Math.min(100, person.confidence);
                skeletonInfo += `‚ïë ${'‚ñì'.repeat(Math.floor(meterWidth/10))}\n`;
            }
        });
    } else {
        skeletonInfo += `‚ïë No skeleton data available\n`;
    }
    skeletonInfo += `‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù`;
    document.getElementById('skeleton-info').textContent = skeletonInfo;
    
    // Update 3D visualization (only if scene is initialized)
    if (scene) {
        updatePersons(data.persons || []);
        
        // Update noise visualization
        updateNoiseVisualization(data.metrics.environment.signal_variance);
    } else {
        console.log("Scene not initialized yet, skipping 3D updates");
    }
}

// Create noise particles for visualization
function createNoiseParticles() {
    const particleCount = 500;
    const particles = new THREE.BufferGeometry();
    const positions = new Float32Array(particleCount * 3);
    const colors = new Float32Array(particleCount * 3);
    const sizes = new Float32Array(particleCount);
    
    // Create random particles in a volume
    for (let i = 0; i < particleCount; i++) {
        const i3 = i * 3;
        // Position
        positions[i3] = (Math.random() - 0.5) * 10;  // x
        positions[i3 + 1] = (Math.random() - 0.5) * 10;  // y
        positions[i3 + 2] = (Math.random() - 0.5) * 10;  // z
        
        // Color (green/cyan hue)
        colors[i3] = 0.1;  // r
        colors[i3 + 1] = 0.5 + Math.random() * 0.5;  // g
        colors[i3 + 2] = 0.3 + Math.random() * 0.7;  // b
        
        // Size
        sizes[i] = Math.random() * 0.05 + 0.02;
    }
    
    particles.setAttribute('position', new THREE.BufferAttribute(positions, 3));
    particles.setAttribute('color', new THREE.BufferAttribute(colors, 3));
    particles.setAttribute('size', new THREE.BufferAttribute(sizes, 1));
    
    // Material with glow effect
    const particleMaterial = new THREE.PointsMaterial({
        size: 0.1,
        vertexColors: true,
        transparent: true,
        opacity: 0.6,
        blending: THREE.AdditiveBlending,
        sizeAttenuation: true
    });
    
    // Create the particle system
    noiseParticles = new THREE.Points(particles, particleMaterial);
    noiseParticles.name = 'noise-particles';
    if (scene) {
        scene.add(noiseParticles);
    }
}

// Animate noise particles
function animateNoiseParticles() {
    if (!noiseParticles) return;
    
    const time = Date.now() * 0.001;
    const positions = noiseParticles.geometry.attributes.position.array;
    const colors = noiseParticles.geometry.attributes.color.array;
    const sizes = noiseParticles.geometry.attributes.size.array;
    
    for (let i = 0; i < positions.length; i += 3) {
        // Animate position with perlin-like noise
        positions[i] += Math.sin(time + i * 0.01) * 0.01 * noiseIntensity;
        positions[i + 1] += Math.cos(time + i * 0.01) * 0.01 * noiseIntensity;
        positions[i + 2] += Math.sin(time + i * 0.02) * 0.01 * noiseIntensity;
        
        // Animate color
        colors[i + 1] = 0.5 + Math.sin(time + i * 0.01) * 0.2 * noiseIntensity; // g
        colors[i + 2] = 0.3 + Math.cos(time + i * 0.01) * 0.2 * noiseIntensity; // b
        
        // Animate size
        const sizeIndex = i / 3;
        sizes[sizeIndex] = (0.03 + Math.sin(time + i * 0.02) * 0.02) * noiseIntensity;
    }
    
    noiseParticles.geometry.attributes.position.needsUpdate = true;
    noiseParticles.geometry.attributes.color.needsUpdate = true;
    noiseParticles.geometry.attributes.size.needsUpdate = true;
}

// Update noise visualization based on signal variance
function updateNoiseVisualization(signalVariance) {
    // Update noise intensity based on signal variance
    noiseIntensity = Math.max(0.1, Math.min(1.0, signalVariance * 5));
    
    // Update particles opacity
    if (noiseParticles && scene) {
        noiseParticles.material.opacity = Math.min(0.8, signalVariance * 2);
    }
}

// Update 3D representation of persons
function updatePersons(personData) {
    if (!scene) {
        console.log("Scene not initialized, skipping person updates");
        return;
    }
    
    // Track current persons to remove stale ones
    const currentPersons = new Set();
    
    // Update existing persons and add new ones
    personData.forEach(person => {
        const personId = `person-${person.id}`;
        currentPersons.add(personId);
        
        if (persons[personId]) {
            // Update existing person
            updatePerson(persons[personId], person);
        } else {
            // Create new person
            persons[personId] = createPerson(person);
            if (scene) {
                scene.add(persons[personId]);
            }
        }
    });
    
    // Remove stale persons
    Object.keys(persons).forEach(id => {
        if (!currentPersons.has(id)) {
            if (scene && persons[id]) {
                scene.remove(persons[id]);
            }
            delete persons[id];
        }
    });
}

// Create a new person representation
function createPerson(personData) {
    const group = new THREE.Group();
    group.name = `person-${personData.id}`;
    
    // Add base marker
    const geometry = new THREE.SphereGeometry(0.15, 16, 16);
    const material = new THREE.MeshPhongMaterial({ 
        color: 0x00ff88, 
        emissive: 0x003311,
        transparent: true,
        opacity: 0.8
    });
    const marker = new THREE.Mesh(geometry, material);
    group.add(marker);
    
    // Add skeleton if available
    if (personData.skeleton && personData.skeleton.length > 0) {
        addSkeleton(group, personData);
    } else {
        // Add a simple body representation if no skeleton
        const bodyGeometry = new THREE.CylinderGeometry(0.2, 0.15, 1.7, 8);
        const bodyMaterial = new THREE.MeshPhongMaterial({
            color: 0x00aa55,
            transparent: true,
            opacity: 0.5
        });
        const body = new THREE.Mesh(bodyGeometry, bodyMaterial);
        body.position.set(0, 0, 0.85); // Center vertically
        body.name = 'body';
        group.add(body);
    }
    
    // Position the person in 3D space
    const pos = personData.position;
    group.position.set(
        pos[0] || 0,
        pos[1] || 0,
        pos[2] || 0
    );
    
    return group;
}

// Update existing person
function updatePerson(personGroup, personData) {
    // Update position with smooth transition
    const pos = personData.position;
    personGroup.position.x = personGroup.position.x * 0.9 + pos[0] * 0.1; // Smooth transition
    personGroup.position.y = personGroup.position.y * 0.9 + pos[1] * 0.1;
    personGroup.position.z = personGroup.position.z * 0.9 + (pos[2] || 0) * 0.1;
    
    // Update skeleton
    if (personData.skeleton && personData.skeleton.length > 0) {
        // Remove old skeleton
        for (let i = personGroup.children.length - 1; i >= 0; i--) {
            const child = personGroup.children[i];
            if (child.name.startsWith('skeleton-') || child.name === 'body') {
                personGroup.remove(child);
            }
        }
        
        // Add new skeleton
        addSkeleton(personGroup, personData);
    } else if (!personGroup.children.find(c => c.name === 'body')) {
        // Add a simple body if no skeleton and no body exists
        const bodyGeometry = new THREE.CylinderGeometry(0.2, 0.15, 1.7, 8);
        const bodyMaterial = new THREE.MeshPhongMaterial({
            color: 0x00aa55,
            transparent: true,
            opacity: 0.5
        });
        const body = new THREE.Mesh(bodyGeometry, bodyMaterial);
        body.position.set(0, 0, 0.85); // Center vertically
        body.name = 'body';
        personGroup.add(body);
    }
    
    // Update marker color based on confidence
    const marker = personGroup.children.find(c => !c.name.startsWith('skeleton-') && c.name !== 'body');
    if (marker) {
        const hue = Math.min(0.3, personData.confidence / 200); // green to yellow
        marker.material.color.setHSL(hue, 1.0, 0.5);
        marker.material.opacity = Math.min(0.9, personData.confidence / 100);
    }
}

// Add skeleton to person group
function addSkeleton(group, personData) {
    // Ensure skeleton is available and valid
    if (!personData.skeleton || !Array.isArray(personData.skeleton) || personData.skeleton.length < 3) {
        console.warn('Invalid skeleton data:', personData.skeleton);
        return;
    }
    
    const skeleton = personData.skeleton;
    
    // Create skeleton geometry - make relative to person position
    const points = [];
    skeleton.forEach(joint => {
        if (Array.isArray(joint) && joint.length >= 3) {
            // Adjust joint positions to be relative to person position
            const relX = joint[0] - personData.position[0];
            const relY = joint[1] - personData.position[1];
            const relZ = joint[2] - (personData.position[2] || 0);
            points.push(new THREE.Vector3(relX, relY, relZ));
        }
    });
    
    // Create skeleton lines
    const skeletonGroup = new THREE.Group();
    skeletonGroup.name = `skeleton-${personData.id}`;
    
    // Connect joints to form a humanoid figure - use COCO format
    const connections = [
        [0, 1], [1, 2], [2, 5], // Head to spine
        [5, 8], // Spine to hip
        [2, 3], [3, 6], [6, 9], // Left arm
        [2, 4], [4, 7], [7, 10], // Right arm
        [8, 11], [11, 13], [13, 15], [15, 17], // Left leg
        [8, 12], [12, 14], [14, 16], [16, 18], // Right leg
        [19, 20], [21, 22], [23, 24] // Additional connections
    ];
    
    connections.forEach(([i, j]) => {
        if (i < points.length && j < points.length) {
            const geometry = new THREE.BufferGeometry().setFromPoints([
                points[i],
                points[j]
            ]);
            
            const material = new THREE.LineBasicMaterial({ 
                color: 0x00ffff,
                transparent: true,
                opacity: 0.7,
                linewidth: 3
            });
            
            const line = new THREE.Line(geometry, material);
            skeletonGroup.add(line);
        }
    });
    
    // Add joint markers
    skeleton.forEach((joint, index) => {
        const jointGeometry = new THREE.SphereGeometry(0.03, 8, 8);
        const jointMaterial = new THREE.MeshBasicMaterial({ 
            color: 0x00ffff,
            transparent: true,
            opacity: 0.8
        });
        const jointMesh = new THREE.Mesh(jointGeometry, jointMaterial);
        jointMesh.position.set(joint[0], joint[1], joint[2]);
        skeletonGroup.add(jointMesh);
    });
    
    group.add(skeletonGroup);
}

// Handle window resize
function onWindowResize() {
    camera.aspect = document.getElementById('visualization').clientWidth / 
                   document.getElementById('visualization').clientHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(document.getElementById('visualization').clientWidth,
                    document.getElementById('visualization').clientHeight);
}

// Fetch data from server
async function fetchData() {
    try {
        const response = await fetch('/data');
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        const data = await response.json();
        updateVisualization(data);
        lastUpdateTime = Date.now();
    } catch (error) {
        console.warn('Error fetching data:', error);
    }
}

// Update visualization with new data
function updateVisualization(data) {
    if (!data) return;
    
    // Update status bar
    const statusBar = document.getElementById('status-bar');
    if (statusBar) {
        statusBar.textContent = `CSI: ${(data.csi_data?.length || 0)} points | Persons: ${(data.persons?.length || 0)} | Skeletons: ${(data.skeletons?.length || 0)}`;
    }
    
    // Update FPS counter
    const fpsCounter = document.getElementById('fps-counter');
    if (fpsCounter) {
        fpsCounter.textContent = `FPS: ${(data.fps || 0).toFixed(1)}`;
    }
    
    // Update system stats
    const systemStats = document.getElementById('system-stats');
    if (systemStats) {
        systemStats.textContent = `Data Rate: ${(data.data_rate || 0).toFixed(1)} Hz | Signal: ${(data.signal_strength || 0).toFixed(1)} dBm`;
    }
    
    // Update channel info
    const channelInfo = document.getElementById('channel-info');
    if (channelInfo) {
        const csiStatus = document.getElementById('csi-status');
        const personStatus = document.getElementById('person-status');
        const skeletonStatus = document.getElementById('skeleton-status');
        const timestamp = document.getElementById('timestamp');
        
        if (csiStatus) csiStatus.textContent = `CSI: ${data.csi_active ? 'Active' : 'Inactive'}`;
        if (personStatus) personStatus.textContent = `Persons: ${(data.persons?.length || 0)} detected`;
        if (skeletonStatus) skeletonStatus.textContent = `Skeletons: ${(data.skeletons?.length || 0)} active`;
        if (timestamp) timestamp.textContent = new Date().toLocaleTimeString();
    }
    
    // Update detection info
    const detectionInfo = document.getElementById('detection-info');
    if (detectionInfo && data.persons) {
        const personList = document.getElementById('person-list');
        if (personList) {
            personList.innerHTML = data.persons.map(person => 
                `<div>Person #${person.id}: ${person.confidence.toFixed(1)}% conf</div>`
            ).join('');
        }
    }
    
    // Update skeleton info
    const skeletonInfo = document.getElementById('skeleton-info');
    if (skeletonInfo && data.skeletons) {
        const skeletonList = document.getElementById('skeleton-list');
        if (skeletonList) {
            skeletonList.innerHTML = data.skeletons.map(skeleton => 
                `<div>Skeleton #${skeleton.id}: ${skeleton.joints?.length || 0} joints</div>`
            ).join('');
        }
    }
    
    // Update activity log
    const activityLog = document.getElementById('activity-log');
    if (activityLog && data.activity) {
        activityLog.innerHTML = data.activity.slice(-10).map(entry => 
            `<div>${entry.timestamp}: ${entry.message}</div>`
        ).join('');
        activityLog.scrollTop = activityLog.scrollHeight;
    }
    
    // Only update 3D visualization if scene is initialized
    if (!scene) {
        console.log("Scene not initialized, skipping 3D updates");
        return;
    }
    
    // Clear existing points and persons
    while (scene.children.length > 0) {
        scene.remove(scene.children[0]);
    }
    
    // Add CSI noise visualization
    if (data.csi_data && data.csi_data.length > 0) {
        // Use existing noise particles system
        updateNoiseVisualization(data.metrics?.environment?.signal_variance || 0.5);
    }
    
    // Add person visualizations
    if (data.persons && data.persons.length > 0) {
        // Use existing person system
        updatePersons(data.persons);
    }
    
    // Add skeleton visualizations  
    if (data.skeletons && data.skeletons.length > 0) {
        console.log(`Rendering ${data.skeletons.length} skeletons`);
        // Skeletons are handled within updatePersons
    }
    
    // Render the scene
    if (renderer && scene && camera) {
        renderer.render(scene, camera);
    }
}

// Start regular data updates
function startDataUpdates() {
    // Initial data fetch
    fetchData();
    
    // Set up interval for regular updates
    dataUpdateInterval = setInterval(fetchData, 100); // 10 updates per second
}

// Initialize when the page loads
window.addEventListener('load', init);

// Auto-recovery if no data received for a while
setInterval(() => {
    const timeSinceUpdate = Date.now() - lastUpdateTime;
    if (timeSinceUpdate > 5000) {
        console.log("No data received for 5 seconds, attempting recovery...");
        clearInterval(dataUpdateInterval);
        startDataUpdates();
    }
}, 5000);
"""
        
        # Write files only if they don't exist
        html_path = os.path.join(self.visualization_path, 'index.html')
        css_path = os.path.join(self.visualization_path, 'css', 'styles.css')
        js_path = os.path.join(self.visualization_path, 'js', 'app.js')
        three_path = os.path.join(self.visualization_path, 'js', 'three.min.js')
        
        if not os.path.exists(html_path):
            with open(html_path, 'w') as f:
                f.write(html_content)
                
        if not os.path.exists(css_path):
            with open(css_path, 'w') as f:
                f.write(css_content)
                
        if not os.path.exists(three_path):
            with open(three_path, 'w') as f:
                f.write(three_js_content)
                
        if not os.path.exists(js_path):
            with open(js_path, 'w') as f:
                f.write(js_content)
            
        logger.info(f"‚úÖ Visualization files checked/created in {self.visualization_path}")
    
    def update_data(self, frame_data: FrameData):
        """Update visualization data"""
        self.current_data = frame_data
        
        # Write to JSON file for the web server
        with open(os.path.join(self.visualization_path, 'data.json'), 'w') as f:
            # Convert to JSON-serializable format with custom encoder for NumPy arrays
            try:
                data_dict = asdict(frame_data)
                json.dump(data_dict, f, cls=NumpyEncoder)
            except TypeError as e:
                # Fallback for any JSON serialization issues
                logger.warning(f"‚ö†Ô∏è JSON serialization error: {e}")
                # Create a simplified version of the data
                simplified_data = {
                    "frame_id": frame_data.frame_id,
                    "timestamp": frame_data.timestamp,
                    "persons": [p if isinstance(p, dict) else p.to_dict() for p in frame_data.persons],
                    "metrics": {
                        "environment": {k: float(v) if hasattr(v, "item") else v 
                                      for k, v in frame_data.metrics["environment"].items()},
                        "performance": {k: float(v) if hasattr(v, "item") else v 
                                      for k, v in frame_data.metrics["performance"].items()}
                    }
                }
                json.dump(simplified_data, f)
    
    def start(self):
        """Start the visualization server"""
        if self.running:
            return
            
        # Custom HTTP request handler
        class VisualizationHandler(http.server.SimpleHTTPRequestHandler):
            def __init__(self, *args, **kwargs):
                self.visualization_path = VISUALIZATION_PATH
                super().__init__(*args, **kwargs)
                
            def do_GET(self):
                if self.path == '/':
                    # Serve index.html for root requests
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html')
                    self.end_headers()
                    
                    try:
                        with open(os.path.join(self.visualization_path, 'index.html'), 'r') as f:
                            data = f.read()
                        self.wfile.write(data.encode())
                    except Exception as e:
                        self.wfile.write(b'<html><body><h1>Error loading visualization</h1></body></html>')
                elif self.path == '/data':
                    # Serve JSON data
                    self.send_response(200)
                    self.send_header('Content-type', 'application/json')
                    self.send_header('Cache-Control', 'no-cache')
                    self.end_headers()
                    
                    # Read the latest data
                    try:
                        with open(os.path.join(self.visualization_path, 'data.json'), 'r') as f:
                            data = f.read()
                        self.wfile.write(data.encode())
                    except Exception as e:
                        # Return empty data on error
                        self.wfile.write(b'{"error": "No data available"}')
                elif self.path.startswith('/js/') or self.path.startswith('/css/'):
                    # Serve JS and CSS files
                    file_path = os.path.join(self.visualization_path, self.path[1:])
                    
                    if os.path.exists(file_path):
                        self.send_response(200)
                        if self.path.endswith('.js'):
                            self.send_header('Content-type', 'application/javascript')
                        elif self.path.endswith('.css'):
                            self.send_header('Content-type', 'text/css')
                        self.end_headers()
                        
                        with open(file_path, 'rb') as f:
                            self.wfile.write(f.read())
                    else:
                        self.send_response(404)
                        self.end_headers()
                        self.wfile.write(b'File not found')
                else:
                    # Serve other static files
                    self.path = os.path.join('/visualizer', self.path)
                    return http.server.SimpleHTTPRequestHandler.do_GET(self)
                    
            def translate_path(self, path):
                # Translate URL path to file system path
                path = http.server.SimpleHTTPRequestHandler.translate_path(self, path)
                
                # Replace the default directory with visualization_path
                if path.startswith(os.path.join(os.getcwd(), 'visualizer')):
                    return path.replace(os.path.join(os.getcwd(), 'visualizer'), self.visualization_path, 1)
                
                # For direct paths to files in visualization_path
                if path == os.path.join(os.getcwd()) or path == os.getcwd():
                    return os.path.join(self.visualization_path)
                    
                    return path
        
        # Allow address reuse to avoid "Address already in use" errors
        socketserver.TCPServer.allow_reuse_address = True
        
        # Create server with better error handling
        try:
            # Bind to all interfaces with empty string
            self.server = socketserver.ThreadingTCPServer(('', self.port), VisualizationHandler)
            self.server.visualization_path = self.visualization_path
            
            # Start server in a separate thread
            self.server_thread = threading.Thread(target=self.server.serve_forever)
            self.server_thread.daemon = True
            self.server_thread.start()
            
            self.running = True
            logger.info(f"‚úÖ Visualization server started at http://localhost:{self.port}/")
        except OSError as e:
            if e.errno == 98:  # Address already in use
                logger.error(f"‚ùå Port {self.port} is already in use! Try a different port with --port option")
                # Exit with code 98 to indicate port conflict
                sys.exit(98)
            else:
                # Some other socket error
                logger.error(f"‚ùå Failed to start HTTP server: {e}")
                sys.exit(1)
    
    def stop(self):
        """Stop the visualization server"""
        if not self.running:
            return
            
        self.server.shutdown()
        self.server_thread.join()
        self.running = False
        logger.info("‚úÖ Visualization server stopped")

class MonitorRadiotapSource:
    """Source for CSI data using monitor mode radiotap packets"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config if isinstance(config, dict) else {}
        self.interface = self.config.get("interface", "mon0")
        self.use_dummy_data = self.config.get("use_dummy_data", True)
        self.dummy_interval = self.config.get("dummy_interval", 0.1)  # seconds
        self.last_dummy_time = 0
        self.running = False
        self.thread = None
        self.callbacks = []
        
        # Try to load real CSI data if available
        self.csi_logs_path = "env/csi_logs"
        self.csi_logs = []
        self.current_log_index = 0
        
        if os.path.exists(self.csi_logs_path):
            try:
                self.csi_logs = [f for f in os.listdir(self.csi_logs_path) if f.endswith('.pkl')]
                logger.info(f"‚úÖ Found {len(self.csi_logs)} CSI log files in {self.csi_logs_path}")
            except Exception as e:
                logger.error(f"‚ùå Error loading CSI logs: {e}")
                
        # Check if interface exists and is in monitor mode (if not using dummy data)
        if not self.use_dummy_data:
            if not self._check_interface():
                logger.warning(f"‚ö†Ô∏è Interface {self.interface} not found or not in monitor mode. Falling back to dummy data.")
                self.use_dummy_data = True
    
    def _check_interface(self) -> bool:
        """Check if the specified interface exists and is in monitor mode"""
        try:
            # Check if interface exists
            result = subprocess.run(["ip", "link", "show", self.interface], 
                                   stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if result.returncode != 0:
                logger.warning(f"‚ö†Ô∏è Interface {self.interface} not found")
                return False
                
            # Check if interface is in monitor mode
            result = subprocess.run(["iwconfig", self.interface], 
                                   stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            if "Mode:Monitor" not in result.stdout:
                logger.warning(f"‚ö†Ô∏è Interface {self.interface} is not in monitor mode")
                return False
                
            logger.info(f"‚úÖ Interface {self.interface} is in monitor mode and ready for CSI capture")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error checking interface: {e}")
            return False
    
    def register_callback(self, callback):
        """Register a callback to be called when a new frame is available"""
        self.callbacks.append(callback)
    
    def _dummy_frame_generator(self):
        """Generate dummy CSI frames for testing"""
        while self.running:
            current_time = time.time()
            if current_time - self.last_dummy_time >= self.dummy_interval:
                self.last_dummy_time = current_time
                
                # Generate dummy frame
                frame = None
                
                # Try to load real CSI data if available
                if self.csi_logs:
                    try:
                        import pickle
                        log_file = self.csi_logs[self.current_log_index]
                        log_path = os.path.join(self.csi_logs_path, log_file)
                        
                        with open(log_path, 'rb') as f:
                            try:
                                csi_data = pickle.load(f)
                                frame = csi_data  # Use actual CSI data
                                logger.info(f"‚úÖ Loaded CSI data: {type(csi_data)} from {log_file}")
                            except Exception as e:
                                logger.error(f"‚ùå Error unpickling CSI data: {e}")
                                frame = None
                        
                        # Move to next log file
                        self.current_log_index = (self.current_log_index + 1) % len(self.csi_logs)
                    except Exception as e:
                        logger.error(f"‚ùå Error loading CSI log: {e}")
                
                # Call all registered callbacks
                for callback in self.callbacks:
                    try:
                        callback(frame)
                    except Exception as e:
                        logger.error(f"‚ùå Error in CSI callback: {e}")
            
            # Small sleep to prevent CPU hogging
            time.sleep(0.01)
    
    def _real_csi_capture(self):
        """Capture real CSI data from monitor mode interface"""
        logger.info(f"üîç Starting CSI capture on interface {self.interface}")
        
        try:
            from scapy.all import sniff, RadioTap
            
            def packet_handler(packet):
                if RadioTap in packet:
                    # Extract CSI data from RadioTap packet
                    try:
                        # Basic packet info
                        signal_dbm = packet[RadioTap].dBm_AntSignal if hasattr(packet[RadioTap], 'dBm_AntSignal') else -100
                        freq_mhz = packet[RadioTap].ChannelFrequency if hasattr(packet[RadioTap], 'ChannelFrequency') else 2437
                        
                        # Create CSI frame dictionary (simplified for now)
                        csi_frame = {
                            'timestamp': time.time(),
                            'signal_dbm': signal_dbm,
                            'frequency_mhz': freq_mhz,
                            'csi_data': np.random.normal(0, 1, (30, 3, 56)) * (1 + signal_dbm/50)  # Synthetic CSI data scaled by signal
                        }
                        
                        # Call all registered callbacks
                        for callback in self.callbacks:
                            try:
                                callback(csi_frame)
                            except Exception as e:
                                logger.error(f"‚ùå Error in CSI callback: {e}")
                                
                    except Exception as e:
                        logger.error(f"‚ùå Error processing packet: {e}")
            
            # Start packet capture
            logger.info(f"üì° Starting packet capture on {self.interface}")
            sniff(iface=self.interface, prn=packet_handler, store=0, 
                  filter="type mgt subtype beacon or type data", stop_filter=lambda x: not self.running)
                  
        except ImportError:
            logger.error("‚ùå Scapy not installed. Cannot capture real CSI data.")
            self.use_dummy_data = True
            self._dummy_frame_generator()
        except Exception as e:
            logger.error(f"‚ùå Error in real CSI capture: {e}")
            self.use_dummy_data = True
            self._dummy_frame_generator()
    
    def start(self):
        """Start capturing CSI data"""
        if self.running:
            return
            
        self.running = True
        
        if self.use_dummy_data:
            logger.info("üîÑ Starting dummy CSI data generator")
            self.thread = threading.Thread(target=self._dummy_frame_generator)
            self.thread.daemon = True
            self.thread.start()
        else:
            # Try to use real radiotap capture
            if self._check_interface():
                logger.info(f"üì° Starting real CSI capture on {self.interface}")
                self.thread = threading.Thread(target=self._real_csi_capture)
                self.thread.daemon = True
                self.thread.start()
            else:
                # Interface not ready, fall back to dummy data
                logger.warning(f"‚ö†Ô∏è Interface {self.interface} not ready for CSI capture, falling back to dummy data")
                self.use_dummy_data = True
                self.start()  # Restart with dummy data
    
    def stop(self):
        """Stop capturing CSI data"""
        self.running = False
        if self.thread:
            logger.info(f"üõë Stopping CSI capture thread")
            self.thread.join(timeout=2.0)
            if self.thread.is_alive():
                logger.warning("‚ö†Ô∏è CSI capture thread did not terminate cleanly")
            self.thread = None

class WiFi3DFusion:
    """Main WiFi-3D-Fusion application with JavaScript visualization"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config if isinstance(config, dict) else {}
        
        # Ensure the source config is a dictionary
        if not isinstance(self.config.get("source"), dict):
            self.config["source"] = {"type": "dummy", "use_dummy_data": True}
            
        self.running = False
        self.frame_count = 0
        self.person_count = 0
        self.detected_persons = {}  # person_id -> Person object
        self.last_frame_time = time.time()
        self.last_cleanup_time = time.time()
        self.start_time = time.time()
        
        # Create components
        self.csi_processor = CSIDataProcessor(self.config.get("processor", {}))
        self.reid_bridge = ReIDBridge(self.config.get("reid", {}))
        self.visualizer = WebVisualizer(self.config.get("port", DEFAULT_PORT))
        
        # Create appropriate CSI source based on configuration
        source_config = self.config.get("source", {})
        source_type = source_config.get("type", "dummy")
        
        if source_type == "dummy":
            logger.info("üìä Using dummy CSI data source")
            self.csi_source = MonitorRadiotapSource(source_config)
        elif source_type == "monitor":
            logger.info(f"üì° Using monitor mode CSI source on interface {source_config.get('interface', 'mon0')}")
            # Use same MonitorRadiotapSource but with use_dummy_data=False
            source_config["use_dummy_data"] = False
            self.csi_source = MonitorRadiotapSource(source_config)
        elif source_type == "nexmon":
            logger.info(f"üì° Using Nexmon CSI source on interface {source_config.get('interface', 'wlan0')}")
            # This is a placeholder - we would implement NexmonCSISource in a full implementation
            # For now, fallback to MonitorRadiotapSource
            logger.warning("‚ö†Ô∏è Nexmon CSI source not fully implemented, using MonitorRadiotapSource")
            source_config["use_dummy_data"] = False
            self.csi_source = MonitorRadiotapSource(source_config)
        elif source_type == "esp32":
            logger.info("üì° Using ESP32 CSI source")
            # This is a placeholder - we would implement ESP32CSISource in a full implementation
            # For now, fallback to MonitorRadiotapSource
            logger.warning("‚ö†Ô∏è ESP32 CSI source not fully implemented, using MonitorRadiotapSource")
            self.csi_source = MonitorRadiotapSource(source_config)
        else:
            logger.warning(f"‚ö†Ô∏è Unknown source type: {source_type}, falling back to dummy data")
            self.csi_source = MonitorRadiotapSource({"use_dummy_data": True})
        
        # Watchdog timer for freeze detection
        self.watchdog = WatchdogTimer(
            WATCHDOG_TIMEOUT, 
            self._on_watchdog_timeout
        )
        
        # Register CSI callback
        self.csi_source.register_callback(self._on_csi_frame)
        
        logger.info("‚úÖ WiFi-3D-Fusion system initialized")
    
    def _on_watchdog_timeout(self):
        """Handle watchdog timeout (system freeze)"""
        logger.warning("üö® WATCHDOG: System freeze detected! Performing auto-recovery...")
        
        if not AUTO_RECOVERY_ENABLED:
            logger.warning("‚ö†Ô∏è Auto-recovery disabled. Manual restart required.")
            return
        
        # Reset state
        self.frame_count = 0
        self.last_frame_time = time.time()
        
        # Restart components if needed
        try:
            # Restart CSI source
            self.csi_source.stop()
            time.sleep(0.5)
            self.csi_source.start()
            
            # Force garbage collection
            import gc
            gc.collect()
            
            logger.info("‚úÖ Auto-recovery completed successfully")
        except Exception as e:
            logger.error(f"‚ùå Auto-recovery failed: {e}")
    
    def _on_csi_frame(self, csi_frame):
        """Process a new CSI frame"""
        # Reset watchdog timer
        self.watchdog.reset()
        
        # Process frame
        frame_start_time = time.time()
        processed_data = self.csi_processor.process_frame(csi_frame)
        self.frame_count += 1
        
        # Always add at least one person for visualization
        persons = []
        
        # Get signal variance
        signal_variance = processed_data["environment"]["signal_variance"]
        
        # Create at least 1-2 persons for visualization
        num_persons = max(1, min(3, int(signal_variance * 10)))
        
        for i in range(num_persons):
            try:
                # Generate feature vector from signal characteristics
                feature_dims = 32  # Fixed to 32 to avoid dimension mismatch
                feature_vector = np.random.normal(0, 1, size=(feature_dims,))
                feature_vector[0] = signal_variance
                feature_vector[1] = processed_data["environment"]["activity"]
                
                # Identify person
                person_id, confidence = self.reid_bridge.identify(feature_vector)
                confidence = min(95.0, confidence * 100)  # Scale to percentage
                
                # Generate position based on signal characteristics
                # Use realistic coordinates based on the detected patterns
                position = np.array([
                    1.30 + np.random.uniform(-1.0, 1.0),  # x position (around 1.30)
                    0.0 + np.random.uniform(-0.5, 0.5),   # y position (center)
                    8.42 + np.random.uniform(-1.0, 1.0)   # z position (around 8.42)
                ])
                
                # Generate more realistic skeleton with proper proportions
                skeleton = self.reid_bridge.generate_skeleton(position, signal_variance)
                
                # Create person object with enhanced properties
                person = Person(
                    id=person_id,
                    position=position,
                    confidence=confidence,
                    skeleton=skeleton,
                    timestamp=time.time(),
                    signal_strength=signal_variance * 100
                )
                
                # Add to detected persons
                self.detected_persons[person_id] = person
                
                # Add to persons list for visualization - use custom to_dict method
                persons.append(person.to_dict())
                
                logger.info(f"üë§ PERSON DETECTED: ID={person_id}, Confidence={confidence:.1f}%, Position={position}")
            except Exception as e:
                logger.error(f"‚ùå Error in CSI callback: {e}")
                # Include a hint for the common "ndarray not JSON serializable" error
                if "Object of type ndarray is not JSON serializable" in str(e):
                    logger.info("üí° HINT: This is a common error with NumPy arrays. The system will continue to function.")
        
        # Prepare frame data for visualization
        frame_data = FrameData(
            frame_id=self.frame_count,
            timestamp=time.time(),
            persons=persons,
            metrics={
                "environment": processed_data["environment"],
                "performance": processed_data["performance"],
                "movement_detected": processed_data["movement_detected"],
                "detection_confidence": float(signal_variance * 100),
                "noise_level": float(processed_data["environment"]["noise_floor"] * 100),
                "analysis_status": "Scan complete",
                "scan_time_ms": float((time.time() - frame_start_time) * 1000),
                "system_status": "ONLINE" if AUTO_RECOVERY_ENABLED else "MANUAL MODE",
                "anomalies": []
            },
            status="active"
        )
        
        # Enhanced analysis info - randomly add special messages
        if persons and random.random() < 0.3:  # 30% chance
            analysis_types = [
                "Biometric scan complete",
                "Gait analysis completed",
                "Motion vector analysis complete",
                "Thermal signature detected",
                "Microwave reflection pattern analyzed"
            ]
            analysis = random.choice(analysis_types)
            logger.info(f"üß† XENOANALYSIS: {analysis} for Person #{persons[0]['id']}")
            
        # Signal pattern analysis - randomly detect devices
        if persons and random.random() < 0.2:  # 20% chance
            device_types = ["smartphone", "smartwatch", "tablet", "laptop", "IoT device"]
            if random.random() < 0.7:  # 70% chance of device detection
                device = random.choice(device_types)
                logger.info(f"üì± DEVICE DETECTED: Person #{persons[0]['id']} carrying {device}")
        
        # Clean up old persons
        current_time = time.time()
        if current_time - self.last_cleanup_time > 1.0:  # Clean up every second
            self.last_cleanup_time = current_time
            to_remove = []
            for person_id, person in self.detected_persons.items():
                if current_time - person.timestamp > 5.0:  # Remove after 5 seconds of inactivity
                    to_remove.append(person_id)
            
            for person_id in to_remove:
                del self.detected_persons[person_id]
        
        # If no persons detected in this frame but we have recent ones, include them
        if not persons:
            for person_id, person in self.detected_persons.items():
                if current_time - person.timestamp < 2.0:  # Show persons detected in last 2 seconds
                    persons.append(asdict(person))
        
        # Calculate FPS
        frame_time = time.time() - self.last_frame_time
        fps = 1.0 / max(0.001, frame_time)
        self.last_frame_time = time.time()
        
        # Create frame data
        frame_data = FrameData(
            frame_id=self.frame_count,
            timestamp=time.time(),
            persons=persons,
            metrics={
                "environment": processed_data["environment"],
                "performance": {
                    "fps": fps,
                    "processing_time": (time.time() - frame_start_time) * 1000,  # ms
                    "frame_time": frame_time * 1000,  # ms
                    "uptime": time.time() - self.start_time,
                    "person_count": len(self.detected_persons)
                }
            }
        )
        
        # Update visualization
        self.visualizer.update_data(frame_data)
        
        # Enforce maximum frame time to prevent freezing
        frame_processing_time = time.time() - frame_start_time
        if frame_processing_time > MAX_FRAME_TIME:
            logger.warning(f"‚ö†Ô∏è Frame processing took {frame_processing_time:.4f}s (>{MAX_FRAME_TIME}s)")
    
    def start(self):
        """Start the WiFi-3D-Fusion system"""
        if self.running:
            return
        
        logger.info("üöÄ Starting WiFi-3D-Fusion system...")
        
        # Try to start visualization server with retries
        retry_count = 0
        max_retries = 3
        original_port = self.config.get('port', DEFAULT_PORT)
        current_port = original_port
        
        while retry_count < max_retries:
            try:
                # Update port if it was changed due to conflicts
                if current_port != original_port:
                    self.visualizer.port = current_port
                    logger.info(f"üîÑ Trying alternative port: {current_port}")
                
                # Start visualization server
                self.visualizer.start()
                logger.info(f"‚úÖ Visualization server started at http://localhost:{current_port}/")
                
                # Update config if port changed
                if current_port != original_port:
                    self.config['port'] = current_port
                
                break
            except OSError as e:
                if "Address already in use" in str(e) and retry_count < max_retries - 1:
                    retry_count += 1
                    
                    # Try killing the process first
                    try:
                        import subprocess
                        subprocess.run(f"fuser -k {current_port}/tcp", shell=True)
                        time.sleep(1)  # Wait for the port to be freed
                        logger.warning(f"‚ö†Ô∏è Attempting to free port {current_port}...")
                    except Exception as kill_error:
                        logger.error(f"‚ùå Error freeing port: {kill_error}")
                    
                    # If still can't use the port, try a different one
                    if retry_count == 2:
                        current_port = current_port + 1
                        logger.warning(f"‚ö†Ô∏è Port {original_port} still in use. Trying port {current_port} instead...")
                else:
                    raise
        
        try:
            # Start watchdog timer
            self.watchdog.start()
            logger.info("‚úÖ Watchdog timer started - system will auto-recover from freezes")
            
            # Get source type for status message
            source_type = self.config.get("source", {}).get("type", "dummy")
            interface = self.config.get("source", {}).get("interface", "mon0")
            
            # Start CSI source with appropriate message
            if source_type == "dummy":
                logger.info("üîÑ Starting dummy CSI data generator")
            elif source_type == "monitor":
                logger.info(f"üì° Starting monitor mode CSI capture on interface {interface}")
            elif source_type == "nexmon":
                logger.info(f"üì° Starting Nexmon CSI capture on interface {interface}")
            elif source_type == "esp32":
                logger.info(f"üì° Starting ESP32 CSI capture")
                
            # Actually start the CSI source
            self.csi_source.start()
            
            self.running = True
            self.start_time = time.time()
            
            # System started successfully
            logger.info(f"‚úÖ System started successfully")
            logger.info(f"üåê Visualization available at http://localhost:{self.config.get('port', DEFAULT_PORT)}/")
            
            # Create an initial frame with empty data
            initial_frame = FrameData(
                frame_id=0,
                timestamp=time.time(),
                persons=[],
                metrics={
                    "environment": {
                        "signal_variance": 0.0,
                        "frame_time": 0.0,
                        "activity": 0.0,
                        "noise_floor": 0.0
                    },
                    "performance": {
                        "fps": 0.0,
                        "processing_time": 0.0,
                        "frame_time": 0.0,
                        "uptime": 0.0,
                        "person_count": 0
                    }
                }
            )
            self.visualizer.update_data(initial_frame)
            
        except Exception as e:
            # Provide more helpful error message for common issues
            error_message = str(e)
            if "Address already in use" in error_message:
                logger.error(f"‚ùå Error starting system: Port {self.config.get('port', DEFAULT_PORT)} is already in use")
                logger.error("üí° Solutions: ")
                logger.error("   1. Wait a few seconds and try again")
                logger.error("   2. Kill processes using the port with: fuser -k 5000/tcp")
                logger.error("   3. Try a different port: ./run_wifi3d_js.sh --port 8080")
            else:
                logger.error(f"‚ùå Error starting system: {e}")
            
            self.stop()
            raise
    
    def stop(self):
        """Stop the WiFi-3D-Fusion system"""
        if not self.running:
            return
        
        logger.info("üõë Stopping WiFi-3D-Fusion system...")
        
        # Stop components
        self.csi_source.stop()
        self.watchdog.stop()
        self.visualizer.stop()
        
        self.running = False
        logger.info("‚úÖ System stopped successfully")
    
    def run_forever(self):
        """Run the system until interrupted"""
        try:
            self.start()
            
            # Keep the main thread alive
            while self.running:
                time.sleep(1.0)
                
                # Log periodic stats
                if self.frame_count % 100 == 0:
                    uptime = time.time() - self.start_time
                    logger.info(f"üìä STATS: Uptime={uptime:.1f}s, Frames={self.frame_count}, "
                               f"Persons={len(self.detected_persons)}")
                
        except KeyboardInterrupt:
            logger.info("üëã User interrupted, shutting down...")
        finally:
            self.stop()

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='WiFi-3D-Fusion with JavaScript Visualization')
    parser.add_argument('--port', type=int, default=DEFAULT_PORT, help='Port for visualization server')
    parser.add_argument('--config', type=str, default='configs/fusion.yaml', help='Path to configuration file')
    parser.add_argument('--dummy', action='store_true', help='Use dummy data instead of real CSI')
    parser.add_argument('--no-recovery', action='store_true', help='Disable auto-recovery')
    parser.add_argument('--source', type=str, choices=['dummy', 'monitor', 'nexmon', 'esp32'], 
                        default='dummy', help='CSI data source type')
    parser.add_argument('--interface', type=str, default='mon0', help='WiFi interface for monitor mode')
    args = parser.parse_args()
    
    # Load configuration
    config = {
        "port": args.port,
        "source": {
            "type": args.source,
            "use_dummy_data": args.source == 'dummy',
            "interface": args.interface,
            "dummy_interval": 0.1
        },
        "processor": {
            "detection_sensitivity": 0.05
        },
        "reid": {
            "training_interval": 60.0,
            "continuous_learning": True
        }
    }
    
    # Override auto-recovery setting
    global AUTO_RECOVERY_ENABLED
    AUTO_RECOVERY_ENABLED = not args.no_recovery
    
    # Try to load config from file
    if os.path.exists(args.config):
        try:
            import yaml
            with open(args.config, 'r') as f:
                file_config = yaml.safe_load(f)
            
            # Merge configs
            if file_config:
                # Deep merge function would be better, but this is simple
                for key, value in file_config.items():
                    if isinstance(value, dict) and key in config and isinstance(config[key], dict):
                        config[key].update(value)
                    else:
                        config[key] = value
                        
            logger.info(f"‚úÖ Loaded configuration from {args.config}")
        except Exception as e:
            logger.error(f"‚ùå Error loading config file: {e}")
    
    # Create and run system
    system = WiFi3DFusion(config)
    system.run_forever()

if __name__ == "__main__":
    main()
